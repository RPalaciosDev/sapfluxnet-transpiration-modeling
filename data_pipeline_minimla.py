#!/usr/bin/env python3
"""
Minimal Data Pipeline - Core Transpiration Features Only
Focus on the 6 key physiological drivers of plant transpiration
"""

import pandas as pd
import numpy as np
import os
import json
import warnings
from datetime import datetime
from pathlib import Path
import argparse

warnings.filterwarnings('ignore')

class MinimalTranspirationPipeline:
    """
    Minimal pipeline focusing only on core transpiration drivers
    """
    
    def __init__(self, input_dir='processed_parquet', output_dir='processed_minimal', 
                 target_col='sap_flow'):
        self.input_dir = Path(input_dir)
        self.output_dir = Path(output_dir)
        self.target_col = target_col
        self.timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # Create output directory
        self.output_dir.mkdir(exist_ok=True)
        
        # Core features (will be updated after running find_core_features.py)
        self.core_features = {
            'Maximum Temperature': 'ta',  # Will be updated
            'Mean Temperature': 'ta',
            'Mean VPD': 'vpd', 
            'Incoming Solar Radiation': 'sw_in',
            'Potential Evapotranspiration': None,  # May not exist
            'Soil Moisture': 'swc_shallow'
        }
        
        print(f"🚀 Minimal Transpiration Pipeline")
        print(f"📁 Input: {input_dir}")
        print(f"📁 Output: {output_dir}")
        print(f"🎯 Target: {target_col}")
        print(f"⚡ Strategy: Core physiological features only")
    
    def load_core_feature_mapping(self):
        """Load core feature mapping from analysis script"""
        try:
            # Try to import the mapping generated by find_core_features.py
            import core_feature_mapping
            self.core_features = core_feature_mapping.CORE_FEATURES
            self.minimal_features = core_feature_mapping.MINIMAL_FEATURES
            print(f"✅ Loaded core feature mapping")
            
            for core_var, feature in self.core_features.items():
                print(f"  {core_var}: {feature}")
                
        except ImportError:
            print(f"⚠️  No core feature mapping found")
            print(f"   Run 'python find_core_features.py' first")
            
            # Use default fallback features
            self.minimal_features = ['ta', 'vpd', 'sw_in', 'swc_shallow']
            print(f"   Using fallback features: {self.minimal_features}")
    
    def process_site(self, parquet_file):
        """Process a single site with minimal features"""
        site_name = parquet_file.stem.replace('_comprehensive', '')
        
        try:
            print(f"  📊 Processing {site_name}...")
            
            # Load data
            df = pd.read_parquet(parquet_file)
            
            if len(df) == 0:
                print(f"    ❌ Empty file")
                return None
            
            # Check for target column
            if self.target_col not in df.columns:
                print(f"    ❌ No target column '{self.target_col}'")
                return None
            
            # Select only core features + target + metadata
            essential_cols = ['TIMESTAMP', 'site']
            if 'plant_id' in df.columns:
                essential_cols.append('plant_id')
            
            # Add available core features
            available_features = []
            for feature in self.minimal_features:
                if feature in df.columns:
                    available_features.append(feature)
                else:
                    print(f"    ⚠️  Missing feature: {feature}")
            
            # Select columns
            selected_cols = essential_cols + available_features + [self.target_col]
            df_minimal = df[selected_cols].copy()
            
            # Basic cleaning
            initial_rows = len(df_minimal)
            
            # Remove rows with missing target
            df_minimal = df_minimal.dropna(subset=[self.target_col])
            
            # Remove rows where ALL core features are missing
            df_minimal = df_minimal.dropna(subset=available_features, how='all')
            
            final_rows = len(df_minimal)
            
            if final_rows == 0:
                print(f"    ❌ No valid data after cleaning")
                return None
            
            print(f"    ✅ {final_rows:,} rows ({initial_rows - final_rows:,} removed)")
            print(f"    📊 Features: {len(available_features)} core + target")
            
            # Save minimal dataset
            output_file = self.output_dir / f"{site_name}_minimal.parquet"
            df_minimal.to_parquet(output_file, index=False)
            
            return {
                'site': site_name,
                'input_rows': initial_rows,
                'output_rows': final_rows,
                'features': available_features,
                'file': str(output_file)
            }
            
        except Exception as e:
            print(f"    ❌ Error processing {site_name}: {e}")
            return None
    
    def run_pipeline(self):
        """Run the minimal processing pipeline"""
        print(f"\n🚀 STARTING MINIMAL PIPELINE")
        print(f"=" * 60)
        
        # Load feature mapping
        self.load_core_feature_mapping()
        
        # Find all parquet files
        parquet_files = list(self.input_dir.glob('*_comprehensive.parquet'))
        
        if not parquet_files:
            print(f"❌ No parquet files found in {self.input_dir}")
            return
        
        print(f"📁 Found {len(parquet_files)} sites to process")
        
        # Process each site
        results = []
        successful = 0
        
        for parquet_file in sorted(parquet_files):
            result = self.process_site(parquet_file)
            if result:
                results.append(result)
                successful += 1
        
        # Save processing summary
        summary = {
            'timestamp': self.timestamp,
            'input_directory': str(self.input_dir),
            'output_directory': str(self.output_dir),
            'core_features': self.core_features,
            'minimal_features': self.minimal_features,
            'total_sites': len(parquet_files),
            'successful_sites': successful,
            'failed_sites': len(parquet_files) - successful,
            'results': results
        }
        
        summary_file = self.output_dir / f'minimal_pipeline_summary_{self.timestamp}.json'
        with open(summary_file, 'w') as f:
            json.dump(summary, f, indent=2)
        
        print(f"\n🎉 MINIMAL PIPELINE COMPLETE")
        print(f"=" * 60)
        print(f"✅ Processed: {successful}/{len(parquet_files)} sites")
        print(f"📁 Output: {self.output_dir}")
        print(f"📊 Core features: {len(self.minimal_features)}")
        print(f"💾 Summary: {summary_file}")
        
        # Show feature usage statistics
        feature_counts = {}
        total_rows = 0
        
        for result in results:
            total_rows += result['output_rows']
            for feature in result['features']:
                feature_counts[feature] = feature_counts.get(feature, 0) + 1
        
        print(f"\n📊 FEATURE USAGE SUMMARY")
        print(f"-" * 40)
        for feature, count in sorted(feature_counts.items()):
            coverage = (count / successful) * 100 if successful > 0 else 0
            print(f"  {feature}: {count}/{successful} sites ({coverage:.1f}%)")
        
        print(f"\n📈 Total data points: {total_rows:,}")
        
        return summary

def main():
    """Main function"""
    parser = argparse.ArgumentParser(description="Minimal Transpiration Pipeline")
    parser.add_argument('--input-dir', default='processed_parquet',
                        help="Directory containing processed parquet files")
    parser.add_argument('--output-dir', default='processed_minimal',
                        help="Directory to save minimal datasets")
    parser.add_argument('--target-col', default='sap_flow',
                        help="Target column name")
    
    args = parser.parse_args()
    
    print("🚀 MINIMAL TRANSPIRATION PIPELINE")
    print("=" * 50)
    print(f"Strategy: Core physiological features only")
    print(f"Input: {args.input_dir}")
    print(f"Output: {args.output_dir}")
    print(f"Target: {args.target_col}")
    print(f"Started: {datetime.now()}")
    
    try:
        pipeline = MinimalTranspirationPipeline(
            input_dir=args.input_dir,
            output_dir=args.output_dir,
            target_col=args.target_col
        )
        
        summary = pipeline.run_pipeline()
        
        if summary and summary['successful_sites'] > 0:
            print(f"\n✅ Pipeline completed successfully!")
            print(f"🎯 Ready for minimal model training")
        else:
            print(f"\n❌ Pipeline failed - no sites processed")
            
    except Exception as e:
        print(f"\n❌ Pipeline failed: {e}")
        import traceback
        traceback.print_exc()
    
    print(f"\nFinished: {datetime.now()}")

if __name__ == "__main__":
    main()
